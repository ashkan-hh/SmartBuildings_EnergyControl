{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from Environment.ipynb\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sympy as sp\n",
    "import ipdb\n",
    "import import_ipynb\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from Environment import Simple1Room\n",
    "from Environment import offline_sim\n",
    "\n",
    "\n",
    "\n",
    "class Stochastic_AC_PG:\n",
    "    \"\"\"\n",
    "    a class for actor-critic stochastic policy-gradient RL\n",
    "    \n",
    "    a Stochastic_AC_PG object is used to implement an actor-critic \n",
    "    stochastic policy-gradient RL with eligibility traces that can \n",
    "    handle both types of average rewards: average reward per time step &\n",
    "    average reward per step\n",
    "    \n",
    "    ...\n",
    "    Attributes\n",
    "    ----------\n",
    "    Td: float\n",
    "        the user-defined desired temperature\n",
    "    learning_rate_policy_mu: float\n",
    "        learning rate for policy mean. It should take a value in [0,1]\n",
    "        (default 0.01)\n",
    "    learning_rate_policy_sigma: float\n",
    "        learning rate for policy variance. It should take a value in \n",
    "        [0,1] (default 0.01)\n",
    "    learning_rate_valuefunc: float\n",
    "        learning rate for state value function. It should take a value\n",
    "        in [0,1] (default 0.01)\n",
    "    learning_rate_avereward: float\n",
    "        learning rate for average reward. It should take a value in\n",
    "        [0,1] (default 0.01)\n",
    "    elig_decay_policy: float\n",
    "        eligibilit decay parameter for policy. It should take a value in\n",
    "        [0,1] (default 0.2)\n",
    "    elig_decay_valuefunc: float\n",
    "        eligibilit decay parameter for state-value function. It should \n",
    "        take a value in [0,1] (default 0.2)\n",
    "    T_upperbound: float\n",
    "        upper limit of switch-OFF temperature. (default Td+5)\n",
    "    T_lowerbound: float\n",
    "        lower limit of switch-ON temperature. (default Td-5)\n",
    "    PerTimeStep: bool\n",
    "        if set to True average reward per time step is used, otherwise\n",
    "        if set to False average reward per step is used (default True)\n",
    "    reward_decay: float\n",
    "        reward decay parameter. It should take a value in [0,1] \n",
    "        (default 1)\n",
    "    n_features: int\n",
    "        number of features for the feature vector (default 2)\n",
    "    \n",
    "    Methods\n",
    "    -------\n",
    "    initialize():\n",
    "        initialized all the system parameters that we want to learn\n",
    "    choose_action(state):\n",
    "        chooses an action (temp. threshold) from a Gaussian distribution\n",
    "    learn(S, A, Sp, R, delta_time):\n",
    "        updates all the system parameters we want to learn\n",
    "    value_func(state):\n",
    "        evaluates the state-value function at a given state\n",
    "    value_func_grad(state):\n",
    "        evaluates the state-value function gradient wrt its parameters \n",
    "        at a given state\n",
    "    policy_func_grad(state, A):\n",
    "        evaluates the policy (its log) gradient wrt its parameters at a \n",
    "        given state and action\n",
    "    feature_vec(cls,state):\n",
    "    this outputs a feature vector\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def __init__(self,\n",
    "                 Td,\n",
    "                 learning_rate_policy_mu=0.01,\n",
    "                 learning_rate_policy_sigma=0.01,\n",
    "                 learning_rate_valuefunc=0.01,\n",
    "                 learning_rate_avereward=0.01,\n",
    "                 elig_decay_policy=0.2,\n",
    "                 elig_decay_valuefunc=0.2,\n",
    "                 T_upperbound=None,\n",
    "                 T_lowerbound=None,\n",
    "                 PerTimeStep=True,\n",
    "                 reward_decay=1,\n",
    "                 n_features=2):\n",
    "\n",
    "        self.Td = Td\n",
    "        self.alpha_theta_mu = learning_rate_policy_mu\n",
    "        self.alpha_theta_sigma = learning_rate_policy_sigma\n",
    "        self.alpha_w = learning_rate_valuefunc\n",
    "        self.eta = learning_rate_avereward\n",
    "        self.gamma = reward_decay\n",
    "        self.lambda_theta = elig_decay_policy\n",
    "        self.lambda_w = elig_decay_valuefunc\n",
    "        self.PerTimeStep = PerTimeStep\n",
    "        self.n_features = n_features\n",
    "        \n",
    "        if T_upperbound is None:\n",
    "            self.Tup = self.Td + 5\n",
    "        else:\n",
    "            self.Tup = T_upperbound\n",
    "\n",
    "        if T_lowerbound is None:\n",
    "            self.Tlow = self.Td - 5\n",
    "        else:\n",
    "            self.Tlow = T_lowerbound\n",
    "\n",
    "    def initialize(self):\n",
    "        \"\"\"\n",
    "        initializes all the parameters.\n",
    "        \n",
    "        initialized values:\n",
    "        theta_sigma = 0.0\n",
    "        theta_mu = [Td-3, Td+3]\n",
    "        w_valuefunc = [np.random.uniform(low=-1, high=1.0, size=None),\n",
    "                       np.random.uniform(low=-1, high=1.0, size=None)]\n",
    "        Rave = 0.0\n",
    "        \n",
    "        all the initial values of eligibility traces are set to 0.0\n",
    "        \"\"\"\n",
    "        \n",
    "        # Initializing parameters of the policy (theta_mu = [theta_ON,\n",
    "        # theta_OFF], theta_sigma) and value function (w=[w_OFF, w_ON]) as well\n",
    "        # as the average reward (Rave)\n",
    "\n",
    "        self.theta_sigma = 0.0\n",
    "        theta_ON = self.Td - 4.0\n",
    "        theta_OFF = self.Td + 4.0\n",
    "        self.theta_mu = np.array([theta_ON, theta_OFF])\n",
    "        self.z_theta_mu = np.array([0.0, 0.0])\n",
    "        self.z_theta_sigma = 0.0\n",
    "        self.z_w = np.array([0.0, 0.0])\n",
    "        self.Rave = 0\n",
    "\n",
    "        w_ON = np.random.uniform(low=-1, high=1.0, size=None)\n",
    "        w_OFF = np.random.uniform(low=-1, high=1.0, size=None)\n",
    "        self.w_valuefunc = np.array([w_OFF, w_ON])\n",
    "\n",
    "        return self.Rave, self.theta_mu, self.theta_sigma, self.w_valuefunc\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        \"\"\"\n",
    "        chooses an action from a Gaussian distribution\n",
    "        \n",
    "        an action, a temperature threshold, is sampled from a Gaussian \n",
    "        distribution whose mean and variance are calculated based on the\n",
    "        theta_mu and theta_sigma and a feature state vector.\n",
    "        \"\"\"\n",
    "        \n",
    "        # choosing action (threshold temperatures)\n",
    "        T, hs, aT, zT = state\n",
    "        feature_vec = Stochastic_AC_PG.feature_vec(state)\n",
    "        mu = np.dot(self.theta_mu, feature_vec)\n",
    "        sigma = np.exp(self.theta_sigma)\n",
    "\n",
    "        while True:\n",
    "            action = np.random.normal(mu, sigma)\n",
    "            if (hs == 1 and action > T\n",
    "                    and action < self.Tup) or (hs == 0 and action < T\n",
    "                                               and action > self.Tlow):\n",
    "                break\n",
    "\n",
    "        return action\n",
    "\n",
    "    def learn(self, S, A, Sp, R, delta_time):\n",
    "        \"\"\"\n",
    "        this function updates all the system parameters we want to learn\n",
    "        \n",
    "        this function takes the initial state (S) and the action taken \n",
    "        at S (A) as well as the next state (Sp) and the reward (R) and\n",
    "        the transition time (delta_time), and use them all to update all\n",
    "        the system parameters we want to learn (those initialized under\n",
    "        the initialization function)\n",
    "        \"\"\"\n",
    "\n",
    "        if self.PerTimeStep:\n",
    "            delt = R - self.Rave * delta_time + self.value_func(\n",
    "                Sp) - self.value_func(S)\n",
    "            self.Rave = self.Rave + self.eta * delt / delta_time\n",
    "        else:\n",
    "            delt = R - self.Rave + self.value_func(Sp) - self.value_func(S)\n",
    "            self.Rave = self.Rave + self.eta * delt\n",
    "\n",
    "        dV_dw = self.value_func_grad(S)\n",
    "        dlnPi_dtheta_mu = self.policy_func_grad(S, A)[0:2]\n",
    "        dlnPi_dtheta_sigma = self.policy_func_grad(S, A)[-1]\n",
    "\n",
    "        self.z_w = self.lambda_w * self.z_w + dV_dw\n",
    "        self.z_theta_mu = self.lambda_theta * self.z_theta_mu + dlnPi_dtheta_mu\n",
    "        self.z_theta_sigma = self.lambda_theta * self.z_theta_sigma +\\\n",
    "                              dlnPi_dtheta_sigma\n",
    "\n",
    "        self.w_valuefunc = self.w_valuefunc + self.alpha_w * delt * self.z_w\n",
    "        self.theta_mu = self.theta_mu +\\\n",
    "                        self.alpha_theta_mu * delt * self.z_theta_mu\n",
    "        self.theta_sigma = self.theta_sigma +\\\n",
    "                           self.alpha_theta_sigma * delt * self.z_theta_sigma\n",
    "\n",
    "        #         ipdb.set_trace()\n",
    "\n",
    "        return self.Rave, self.theta_mu, self.theta_sigma, self.w_valuefunc\n",
    "\n",
    "    def value_func(self, state):\n",
    "        \"\"\"\n",
    "        this function evaluates the state-value function at a given\n",
    "        state\n",
    "        \"\"\"\n",
    "        \n",
    "        T, hs, aT, zT = state\n",
    "        feature_vec = Stochastic_AC_PG.feature_vec(state)\n",
    "        return np.dot(self.w_valuefunc, feature_vec)\n",
    "\n",
    "    def value_func_grad(self, state):\n",
    "        \"\"\"\n",
    "        this function evaluates the state-value function gradient wrt\n",
    "        its parameters at a given state\n",
    "        \"\"\"\n",
    "        \n",
    "        T, hs, aT, zT = state\n",
    "        feature_vec = Stochastic_AC_PG.feature_vec(state)\n",
    "        dVdw = feature_vec\n",
    "        return dVdw\n",
    "\n",
    "    def policy_func_grad(self, state, A):\n",
    "        \"\"\"\n",
    "        this function evaluates the policy (its log) gradient wrt\n",
    "        its parameters at a given state and action\n",
    "        \"\"\"\n",
    "        \n",
    "        T, hs, aT, zT = state\n",
    "        feature_vec = Stochastic_AC_PG.feature_vec(state)\n",
    "        mu = np.dot(self.theta_mu, feature_vec)\n",
    "        sigma = np.exp(self.theta_sigma)\n",
    "\n",
    "        #         ipdb.set_trace()\n",
    "\n",
    "        m, s = sp.symbols('m s')\n",
    "        lnPi = sp.log(\n",
    "            (1 / (s * sp.sqrt(2 * sp.pi))) * sp.exp(-(A - m)**2 / (2 * s**2)))\n",
    "\n",
    "        dlnPi_dmu = sp.diff(lnPi, m)\n",
    "        dlnPi_dsigma = sp.diff(lnPi, s)\n",
    "\n",
    "        dlnPi_dmu_calc = sp.lambdify((m, s), dlnPi_dmu, 'numpy')\n",
    "        dlnPi_dsigma_calc = sp.lambdify((m, s), dlnPi_dsigma, 'numpy')\n",
    "\n",
    "        dlnPi_dtheta = np.array([dlnPi_dmu_calc(mu, sigma) * (1-hs),\\\n",
    "                                 dlnPi_dmu_calc(mu, sigma) * hs,\\\n",
    "                                 dlnPi_dsigma_calc(mu, sigma) * sigma])\n",
    "        return dlnPi_dtheta\n",
    "\n",
    "    @classmethod\n",
    "    def feature_vec(cls, state):\n",
    "        \"\"\" this outputs a feature vector\"\"\"\n",
    "        \n",
    "        T, hs, aT, zT = state\n",
    "        return np.array([1 - hs, hs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class COPDAC_Q:\n",
    "    \"\"\"\n",
    "    a class for compatible off-policy deterministic actor-critic RL \n",
    "    \n",
    "    COPDAC_Q class is similar to COPDAC_Q0 with the added capability of\n",
    "    handeling fixed-transition times.\n",
    "    \n",
    "    a COPDAC_Q object is used to implement compatible off-policy \n",
    "    deterministic actor-critic RL with simple Q-learning that can handle\n",
    "    both types of average rewards: average reward per time step &\n",
    "    average reward per step\n",
    "    \n",
    "    ...\n",
    "    Attributes\n",
    "    ----------\n",
    "    Td: float\n",
    "        the user-defined desired temperature\n",
    "    learning_rate_policy_mu: float\n",
    "        learning rate for policy. It should take a value in [0,1]\n",
    "        (default 0.01)\n",
    "    learning_rate_baselinefunc: float\n",
    "        learning rate for baseline function. It should take a value in \n",
    "        [0,1] (default 0.01)\n",
    "    learning_rate_actionvaluefunc: float\n",
    "        learning rate for action-value function. It should take a value\n",
    "        in [0,1] (default 0.01)\n",
    "    learning_rate_avereward: float\n",
    "        learning rate for average reward. It should take a value in\n",
    "        [0,1] (default 0.01)\n",
    "    policy_update_freq: int\n",
    "        defines the policy update freq. The policy is updated every\n",
    "        policy_update_freq times (default 1)\n",
    "    T_upperbound: float\n",
    "        upper limit of switch-OFF temperature. (default Td+5)\n",
    "    T_lowerbound: float\n",
    "        lower limit of switch-ON temperature. (default Td-5)\n",
    "    theta_init: float\n",
    "        initial values of [theta_ON, theta_OFF]. (default [Td-2, Td+2])\n",
    "    PerTimeStep: bool\n",
    "        if set to True, average reward per time step is used, otherwise\n",
    "        if set to False, average reward per step is used (default True)\n",
    "    reward_decay: float\n",
    "        reward decay parameter. It should take a value in [0,1] \n",
    "        (default 1)\n",
    "    \n",
    "    \n",
    "    Methods\n",
    "    -------\n",
    "    initialize():\n",
    "        initializes all the system parameters that we want to learn\n",
    "    choose_action(state):\n",
    "        chooses a deterministic action (temp. threshold) with some added noise\n",
    "        for exploration\n",
    "    learn(S, A, Sp, R, delta_time):\n",
    "        updates all the system parameters we want to learn\n",
    "     symbols_list():\n",
    "        defines symboles for the parameters of interes\n",
    "    mu_func_sym():\n",
    "        defines policy function symbolically\n",
    "    mu_grad_sym():\n",
    "        defines symbolically gradient of the policy function wrt its parameters\n",
    "    V_func_sym():\n",
    "        defines symbolically the baseline function\n",
    "    V_grad_sym():\n",
    "        defines symbolically gradient of the baseline function\n",
    "    Q_func_sym():\n",
    "        defines symbolically the action-value function\n",
    "    Q_grad_a_sym():\n",
    "        defines symbolically gradient (wrt action) of the action-value function\n",
    "    Q_grad_w_sym():\n",
    "        defines symbolically gradient (wrt its parameters) of the action-value function\n",
    "        \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # This is for 1Simple room with heating only and with capability of both \n",
    "    # variable and fixed transition time periods/intervals\n",
    "    \n",
    "    # in this class I coded in a more general way using symbolic form for\n",
    "    # function and did everything symbolically using sympy package\n",
    "\n",
    "\n",
    "    def __init__(self,\n",
    "                 Td,\n",
    "                 learning_rate_policy_mu=0.01,\n",
    "                 learning_rate_baselinefunc=0.01,\n",
    "                 learning_rate_actionvaluefunc=0.01,\n",
    "                 learning_rate_avereward=0.01,\n",
    "                 policy_update_freq=1,\n",
    "                 T_upperbound=None,\n",
    "                 T_lowerbound=None,\n",
    "                 theta_init=None,\n",
    "                 PerTimeStep=True,\n",
    "                 reward_decay=1):\n",
    "\n",
    "        self.Td = Td\n",
    "        self.alpha_theta = learning_rate_policy_mu\n",
    "        self.alpha_v = learning_rate_baselinefunc\n",
    "        self.alpha_w = learning_rate_actionvaluefunc\n",
    "        self.eta = learning_rate_avereward\n",
    "        self.gamma = reward_decay\n",
    "        self.PerTimeStep = PerTimeStep\n",
    "        self.policy_update_freq = policy_update_freq\n",
    "        \n",
    "\n",
    "        if T_upperbound is None:\n",
    "            self.Tup = self.Td + 5\n",
    "        else:\n",
    "            self.Tup = T_upperbound\n",
    "\n",
    "        if T_lowerbound is None:\n",
    "            self.Tlow = self.Td - 5\n",
    "        else:\n",
    "            self.Tlow = T_lowerbound\n",
    "\n",
    "        if theta_init is None:\n",
    "            self.theta_init = np.array([self.Td - 2.0, self.Td + 2.0])\n",
    "        else:\n",
    "            self.theta_init = np.asarray(theta_init)\n",
    "\n",
    "\n",
    "    def initialize(self):\n",
    "        \"\"\"initializes parameters of interest\"\"\"\n",
    "        \n",
    "        # first we lambdify some of the useful functions\n",
    "        self.mu, self.V, self.Q, self.dmu_dtheta, self.dV_dv, self.dQ_da, self.dQ_dw = self.lambdify_funcs(\n",
    "        )\n",
    "\n",
    "        self.theta = self.theta_init\n",
    "        self.w = np.random.uniform(low=-1,\n",
    "                                   high=1.0,\n",
    "                                   size=self.mu_grad_sym().shape[0])\n",
    "        self.v = np.random.uniform(low=-1,\n",
    "                                   high=1.0,\n",
    "                                   size=self.V_grad_sym().shape[0])\n",
    "        self.Rave = 0.0\n",
    "        self.policy_update_ind = 0\n",
    "\n",
    "        return self.Rave, self.theta, self.w, self.v\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        \"\"\"\n",
    "        samples an acceptable action (temp. threshold)\n",
    "        \n",
    "        action is chosen based on the deterministic policy with some \n",
    "        added noise for exploration. Here noise is chosen unformly \n",
    "        randomly from a range e.g. [-1.5 1.5]\n",
    "        \"\"\"\n",
    "        \n",
    "        # choosing action (threshold temperature and CO2 density)\n",
    "        T, hs, aT, zT = state\n",
    "        mu = self.mu(state, self.theta)\n",
    "\n",
    "        while True:\n",
    "            Tth = mu[0, 0] + np.random.uniform(\n",
    "                low=-1.5, high=1.5, size=None)\n",
    "            if (hs == 1 and Tth > T\n",
    "                    and Tth <= self.Tup) or (hs == 0 and Tth < T\n",
    "                                             and Tth >= self.Tlow):\n",
    "                break\n",
    "\n",
    "        action = Tth\n",
    "        return action\n",
    "\n",
    "    def learn(self, S, A, Sp, R, delta_time):\n",
    "        \"\"\"\n",
    "        this function updates all the system parameters we want to learn\n",
    "        \n",
    "        this function takes the initial state (S) and the action taken \n",
    "        at S (A) as well as the next state (Sp), the reward (R), and\n",
    "        the transition time (delta_time), and use them all to update all\n",
    "        the system parameters we want to learn (those initialized under\n",
    "        the initialization function)\n",
    "        \"\"\"\n",
    "\n",
    "        self.policy_update_ind = self.policy_update_ind + 1\n",
    "\n",
    "        V_S_v = self.V(S, self.v).item()\n",
    "        V_Sp_v = self.V(Sp, self.v).item()\n",
    "        Q_S_A_theta_v_w = self.Q(S, (A, ), self.theta, self.v, self.w).item()\n",
    "        Q_Sp_muSp_theta_v_w = self.Q(Sp, (self.mu(Sp, self.theta), ),\n",
    "                                     self.theta, self.v, self.w).item()\n",
    "        dmu_dtheta = self.dmu_dtheta(S, self.theta)\n",
    "        dQ_da = self.dQ_da(S, (A, ), self.theta, self.v, self.w)\n",
    "        dQ_dw = self.dQ_dw(S, (A, ), self.theta, self.v, self.w)\n",
    "        dV_dv = self.dV_dv(S, self.v)\n",
    "\n",
    "        if self.PerTimeStep: # shouldn't you change it to \"pertimestep\"\n",
    "            delt = R - self.Rave * delta_time + Q_Sp_muSp_theta_v_w - Q_S_A_theta_v_w\n",
    "            self.Rave = self.Rave + self.eta * delt / delta_time\n",
    "        else:\n",
    "            delt = R - self.Rave + Q_Sp_muSp_theta_v_w - Q_S_A_theta_v_w\n",
    "            self.Rave = self.Rave + self.eta * delt\n",
    "\n",
    "        if self.policy_update_ind % self.policy_update_freq == 0:\n",
    "            self.theta = self.theta + self.alpha_theta * np.matmul(\n",
    "                dmu_dtheta, dQ_da).ravel()\n",
    "\n",
    "        self.w = self.w + self.alpha_w * delt * dQ_dw.ravel()\n",
    "        self.v = self.v + self.alpha_v * delt * dV_dv.ravel()\n",
    "\n",
    "        return self.Rave, self.theta, self.w, self.v\n",
    "\n",
    "    def symbols_list(self):\n",
    "        \"\"\"defines symboles for the parameters of interest\"\"\"\n",
    "        \n",
    "        s = sp.symbols('T, hs, aT, zT')\n",
    "        theta = sp.symbols('theta1:3')\n",
    "        v = sp.symbols('v1:4')\n",
    "        a = sp.symbols('a1:2')\n",
    "        w = sp.symbols('w1:{}'.format(len(theta) + 1))\n",
    "        return s, theta, v, a, w\n",
    "\n",
    "    def mu_func_sym(self):\n",
    "        \"\"\"defines policy function symbolically\"\"\"\n",
    "        \n",
    "        s, theta, v, a, w = self.symbols_list()\n",
    "        T, hs, aT, zT = s\n",
    "        mu = (theta[0] * (1 - hs) + theta[1] * hs) * zT + (aT) * (1 - zT)\n",
    "        mu_vec = sp.Matrix([mu])\n",
    "        return mu_vec\n",
    "\n",
    "    def mu_grad_sym(self):\n",
    "        \"\"\"defines symbolically gradient of the policy function wrt its parameters\"\"\"\n",
    "        \n",
    "        s, theta, v, a, w = self.symbols_list()\n",
    "        mu = self.mu_func_sym()\n",
    "        theta_vec = sp.Matrix([item for item in theta])\n",
    "        mu_jacob_theta = mu.jacobian(theta).T\n",
    "        return mu_jacob_theta\n",
    "\n",
    "    def V_func_sym(self):\n",
    "        \"\"\"defines symbolically the baseline function\"\"\"\n",
    "        \n",
    "        s, theta, v, a, w = self.symbols_list()\n",
    "        T, hs, aT, zT = s\n",
    "        v_vec = sp.Matrix([item for item in v])\n",
    "        feature_vec = sp.Matrix([hs, 1 - hs, T - self.Td])\n",
    "        V = v_vec.T * feature_vec\n",
    "        return V\n",
    "\n",
    "    def V_grad_sym(self):\n",
    "        \"\"\"defines symbolically gradient of the baseline function\"\"\"\n",
    "        \n",
    "        s, theta, v, a, w = self.symbols_list()\n",
    "        V = self.V_func_sym()\n",
    "        v_vec = sp.Matrix([item for item in v])\n",
    "        V_grad = V.jacobian(v_vec).T\n",
    "        return V_grad\n",
    "\n",
    "    def Q_func_sym(self):\n",
    "        \"\"\"defines symbolically the action-value function\"\"\"\n",
    "        \n",
    "        s, theta, v, a, w = self.symbols_list()\n",
    "        a = sp.Matrix([item for item in a])\n",
    "        w = sp.Matrix([item for item in w])\n",
    "        V = self.V_func_sym()\n",
    "        mu = self.mu_func_sym()\n",
    "        dmu_dtheta = self.mu_grad_sym()\n",
    "        Q = (a - mu).T * dmu_dtheta.T * w + V\n",
    "        return Q\n",
    "\n",
    "    def Q_grad_a_sym(self):\n",
    "        \"\"\"defines symbolically gradient (wrt action) of the action-value function\"\"\"\n",
    "        \n",
    "        s, theta, v, a, w = self.symbols_list()\n",
    "        a = sp.Matrix([item for item in a])\n",
    "        Q = self.Q_func_sym()\n",
    "        dQ_da = Q.jacobian(a).T\n",
    "        return dQ_da\n",
    "\n",
    "    def Q_grad_w_sym(self):\n",
    "        \"\"\"\n",
    "        defines symbolically gradient (wrt its parameters) of the action-value\n",
    "        function\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        s, theta, v, a, w = self.symbols_list()\n",
    "        w = sp.Matrix([item for item in w])\n",
    "        Q = self.Q_func_sym()\n",
    "        dQ_dw = Q.jacobian(w).T\n",
    "        return dQ_dw\n",
    "\n",
    "    def lambdify_funcs(self):\n",
    "        \"\"\"Lambdifies the functions of interest with numpy as the target library\"\"\"\n",
    "        \n",
    "        s, theta, v, a, w = self.symbols_list()\n",
    "        mu_lambdified = sp.lambdify([s, theta], self.mu_func_sym(), 'numpy')\n",
    "        V_lambdified = sp.lambdify([s, v], self.V_func_sym(), 'numpy')\n",
    "        Q_lambdified = sp.lambdify([s, a, theta, v, w], self.Q_func_sym(),\n",
    "                                   'numpy')\n",
    "        mu_grad_lambdified = sp.lambdify([s, theta], self.mu_grad_sym(),\n",
    "                                         'numpy')\n",
    "        V_grad_lambdified = sp.lambdify([s, v], self.V_grad_sym(), 'numpy')\n",
    "        Q_grad_a_lambdified = sp.lambdify([s, a, theta, v, w],\n",
    "                                          self.Q_grad_a_sym(), 'numpy')\n",
    "        Q_grad_w_lambdified = sp.lambdify([s, a, theta, v, w],\n",
    "                                          self.Q_grad_w_sym(), 'numpy')\n",
    "        return mu_lambdified, V_lambdified, Q_lambdified, mu_grad_lambdified,\\\n",
    "               V_grad_lambdified, Q_grad_a_lambdified, Q_grad_w_lambdified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from Environment.ipynb\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "import numpy as np\n",
    "import sympy as sp\n",
    "from Environment import Simple1Room\n",
    "class COPDAC_Q_Ton:\n",
    "    \"\"\"\n",
    "    a class for compatible off-policy deterministic actor-critic RL \n",
    "    \n",
    "    COPDAC_Q_Ton class is similar to COPDAC_Q with only optimizing for Ton (Toff is constant and given).\n",
    "    \n",
    "    a COPDAC_Q object is used to implement compatible off-policy \n",
    "    deterministic actor-critic RL with simple Q-learning that can handle\n",
    "    both types of average rewards: average reward per time step &\n",
    "    average reward per step\n",
    "    \n",
    "    ...\n",
    "    Attributes\n",
    "    ----------\n",
    "    Td: float\n",
    "        the user-defined desired temperature\n",
    "    learning_rate_policy_mu: float\n",
    "        learning rate for policy. It should take a value in [0,1]\n",
    "        (default 0.01)\n",
    "    learning_rate_baselinefunc: float\n",
    "        learning rate for baseline function. It should take a value in \n",
    "        [0,1] (default 0.01)\n",
    "    learning_rate_actionvaluefunc: float\n",
    "        learning rate for action-value function. It should take a value\n",
    "        in [0,1] (default 0.01)\n",
    "    learning_rate_avereward: float\n",
    "        learning rate for average reward. It should take a value in\n",
    "        [0,1] (default 0.01)\n",
    "    policy_update_freq: int\n",
    "        defines the policy update freq. The policy is updated every\n",
    "        policy_update_freq times (default 1)\n",
    "    T_upperbound: float\n",
    "        upper limit of switch-OFF temperature. (default Td+5)\n",
    "    T_lowerbound: float\n",
    "        lower limit of switch-ON temperature. (default Td-5)\n",
    "    theta_init: float\n",
    "        initial values of [theta_ON, theta_OFF]. (default [Td-2, Td+2])\n",
    "    PerTimeStep: bool\n",
    "        if set to True, average reward per time step is used, otherwise\n",
    "        if set to False, average reward per step is used (default True)\n",
    "    reward_decay: float\n",
    "        reward decay parameter. It should take a value in [0,1] \n",
    "        (default 1)\n",
    "    \n",
    "    \n",
    "    Methods\n",
    "    -------\n",
    "    initialize():\n",
    "        initializes all the system parameters that we want to learn\n",
    "    choose_action(state):\n",
    "        chooses a deterministic action (temp. threshold) with some added noise\n",
    "        for exploration\n",
    "    learn(S, A, Sp, R, delta_time):\n",
    "        updates all the system parameters we want to learn\n",
    "     symbols_list():\n",
    "        defines symboles for the parameters of interes\n",
    "    mu_func_sym():\n",
    "        defines policy function symbolically\n",
    "    mu_grad_sym():\n",
    "        defines symbolically gradient of the policy function wrt its parameters\n",
    "    V_func_sym():\n",
    "        defines symbolically the baseline function\n",
    "    V_grad_sym():\n",
    "        defines symbolically gradient of the baseline function\n",
    "    Q_func_sym():\n",
    "        defines symbolically the action-value function\n",
    "    Q_grad_a_sym():\n",
    "        defines symbolically gradient (wrt action) of the action-value function\n",
    "    Q_grad_w_sym():\n",
    "        defines symbolically gradient (wrt its parameters) of the action-value function\n",
    "        \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # This is for 1Simple room with heating only and with capability of both \n",
    "    # variable and fixed transition time periods/intervals\n",
    "    \n",
    "    # in this class I coded in a more general way using symbolic form for\n",
    "    # function and did everything symbolically using sympy package\n",
    "\n",
    "\n",
    "    def __init__(self,\n",
    "                 Td,\n",
    "                 Toff = 16.0,\n",
    "                 learning_rate_policy_mu=0.01,\n",
    "                 learning_rate_baselinefunc=0.01,\n",
    "                 learning_rate_actionvaluefunc=0.01,\n",
    "                 learning_rate_avereward=0.01,\n",
    "                 policy_update_freq=1,\n",
    "                 T_upperbound=None,\n",
    "                 T_lowerbound=None,\n",
    "                 theta_init=None,\n",
    "                 PerTimeStep=True,\n",
    "                 reward_decay=1):\n",
    "\n",
    "        self.Td = Td\n",
    "        self.alpha_theta = learning_rate_policy_mu\n",
    "        self.alpha_v = learning_rate_baselinefunc\n",
    "        self.alpha_w = learning_rate_actionvaluefunc\n",
    "        self.eta = learning_rate_avereward\n",
    "        self.gamma = reward_decay\n",
    "        self.PerTimeStep = PerTimeStep\n",
    "        self.policy_update_freq = policy_update_freq\n",
    "        \n",
    "        self.Toff = Toff\n",
    "        \n",
    "\n",
    "        if T_upperbound is None:\n",
    "            self.Tup = self.Td + 5\n",
    "        else:\n",
    "            self.Tup = T_upperbound\n",
    "\n",
    "        if T_lowerbound is None:\n",
    "            self.Tlow = self.Td - 7\n",
    "        else:\n",
    "            self.Tlow = T_lowerbound\n",
    "\n",
    "        if theta_init is None:\n",
    "            self.theta_init = np.array([self.Td - 5.0])\n",
    "        else:\n",
    "            self.theta_init = np.asarray(theta_init)\n",
    "\n",
    "\n",
    "    def initialize(self):\n",
    "        \"\"\"initializes parameters of interest\"\"\"\n",
    "        \n",
    "        # first we lambdify some of the useful functions\n",
    "        self.mu, self.V, self.Q, self.dmu_dtheta, self.dV_dv, self.dQ_da, self.dQ_dw = self.lambdify_funcs(\n",
    "        )\n",
    "\n",
    "        self.theta = self.theta_init\n",
    "        self.w = np.random.uniform(low=-1,\n",
    "                                   high=1.0,\n",
    "                                   size=self.mu_grad_sym().shape[0])\n",
    "        self.v = np.random.uniform(low=-1,\n",
    "                                   high=1.0,\n",
    "                                   size=self.V_grad_sym().shape[0])\n",
    "        self.Rave = 0.0\n",
    "        self.policy_update_ind = 0\n",
    "\n",
    "        return self.Rave, self.theta, self.w, self.v\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        \"\"\"\n",
    "        samples an acceptable action (temp. threshold)\n",
    "        \n",
    "        action is chosen based on the deterministic policy with some \n",
    "        added noise for exploration. Here noise is chosen unformly \n",
    "        randomly from a range e.g. [-1.5 1.5]\n",
    "        \"\"\"\n",
    "        \n",
    "        # choosing action (threshold temperature and CO2 density)\n",
    "        T, hs, aT, zT = state\n",
    "        mu = self.mu(state, self.theta)\n",
    "\n",
    "        while True:\n",
    "            Tth = mu[0, 0] + np.random.uniform(\n",
    "                low=-1.5, high=1.5, size=None)\n",
    "            if (hs == 1 and Tth > T\n",
    "                    and Tth <= self.Tup) or (hs == 0 and Tth < T\n",
    "                                             and Tth >= self.Tlow):\n",
    "                break\n",
    "\n",
    "        action = Tth\n",
    "        return action\n",
    "\n",
    "    def learn(self, S, A, Sp, R, delta_time):\n",
    "        \"\"\"\n",
    "        this function updates all the system parameters we want to learn\n",
    "        \n",
    "        this function takes the initial state (S) and the action taken \n",
    "        at S (A) as well as the next state (Sp), the reward (R), and\n",
    "        the transition time (delta_time), and use them all to update all\n",
    "        the system parameters we want to learn (those initialized under\n",
    "        the initialization function)\n",
    "        \"\"\"\n",
    "\n",
    "        self.policy_update_ind = self.policy_update_ind + 1\n",
    "\n",
    "        V_S_v = self.V(S, self.v).item()\n",
    "        V_Sp_v = self.V(Sp, self.v).item()\n",
    "        Q_S_A_theta_v_w = self.Q(S, (A, ), self.theta, self.v, self.w).item()\n",
    "        Q_Sp_muSp_theta_v_w = self.Q(Sp, (self.mu(Sp, self.theta), ),\n",
    "                                     self.theta, self.v, self.w).item()\n",
    "        dmu_dtheta = self.dmu_dtheta(S, self.theta)\n",
    "        dQ_da = self.dQ_da(S, (A, ), self.theta, self.v, self.w)\n",
    "        dQ_dw = self.dQ_dw(S, (A, ), self.theta, self.v, self.w)\n",
    "        dV_dv = self.dV_dv(S, self.v)\n",
    "\n",
    "        if self.PerTimeStep: # shouldn't you change it to \"pertimestep\"\n",
    "            delt = R - self.Rave * delta_time + Q_Sp_muSp_theta_v_w - Q_S_A_theta_v_w\n",
    "            self.Rave = self.Rave + self.eta * delt / delta_time\n",
    "        else:\n",
    "            delt = R - self.Rave + Q_Sp_muSp_theta_v_w - Q_S_A_theta_v_w\n",
    "            self.Rave = self.Rave + self.eta * delt\n",
    "\n",
    "        if self.policy_update_ind % self.policy_update_freq == 0:\n",
    "            self.theta = self.theta + self.alpha_theta * np.matmul(\n",
    "                dmu_dtheta, dQ_da).ravel()\n",
    "\n",
    "        self.w = self.w + self.alpha_w * delt * dQ_dw.ravel()\n",
    "        self.v = self.v + self.alpha_v * delt * dV_dv.ravel()\n",
    "\n",
    "        return self.Rave, self.theta, self.w, self.v\n",
    "\n",
    "    def symbols_list(self):\n",
    "        \"\"\"defines symboles for the parameters of interest\"\"\"\n",
    "        \n",
    "        s = sp.symbols('T, hs, aT, zT')\n",
    "        theta = sp.symbols('theta1:2')\n",
    "        #         v = sp.symbols('v1:3')\n",
    "        v = sp.symbols('v1:4')\n",
    "        a = sp.symbols('a1:2')\n",
    "        w = sp.symbols('w1:{}'.format(len(theta) + 1))\n",
    "        return s, theta, v, a, w\n",
    "\n",
    "    def mu_func_sym(self):\n",
    "        \"\"\"defines policy function symbolically\"\"\"\n",
    "        \n",
    "        s, theta, v, a, w = self.symbols_list()\n",
    "        T, hs, aT, zT = s\n",
    "        mu = (theta[0] * (1 - hs) + self.Toff * hs) * zT + (aT) * (1 - zT)\n",
    "        mu_vec = sp.Matrix([mu])\n",
    "        return mu_vec\n",
    "\n",
    "    def mu_grad_sym(self):\n",
    "        \"\"\"defines symbolically gradient of the policy function wrt its parameters\"\"\"\n",
    "        \n",
    "        s, theta, v, a, w = self.symbols_list()\n",
    "        mu = self.mu_func_sym()\n",
    "        theta_vec = sp.Matrix([item for item in theta])\n",
    "        mu_jacob_theta = mu.jacobian(theta).T\n",
    "        return mu_jacob_theta\n",
    "\n",
    "    def V_func_sym(self):\n",
    "        \"\"\"defines symbolically the baseline function\"\"\"\n",
    "        \n",
    "        s, theta, v, a, w = self.symbols_list()\n",
    "        T, hs, aT, zT = s\n",
    "        v_vec = sp.Matrix([item for item in v])\n",
    "        #         feature_vec = sp.Matrix([hs, (1-hs)])\n",
    "        feature_vec = sp.Matrix([hs, 1 - hs, T - self.Td])\n",
    "        V = v_vec.T * feature_vec\n",
    "        return V\n",
    "\n",
    "    def V_grad_sym(self):\n",
    "        \"\"\"defines symbolically gradient of the baseline function\"\"\"\n",
    "        \n",
    "        s, theta, v, a, w = self.symbols_list()\n",
    "        V = self.V_func_sym()\n",
    "        v_vec = sp.Matrix([item for item in v])\n",
    "        V_grad = V.jacobian(v_vec).T\n",
    "        return V_grad\n",
    "\n",
    "    def Q_func_sym(self):\n",
    "        \"\"\"defines symbolically the action-value function\"\"\"\n",
    "        \n",
    "        s, theta, v, a, w = self.symbols_list()\n",
    "        a = sp.Matrix([item for item in a])\n",
    "        w = sp.Matrix([item for item in w])\n",
    "        V = self.V_func_sym()\n",
    "        mu = self.mu_func_sym()\n",
    "        dmu_dtheta = self.mu_grad_sym()\n",
    "        Q = (a - mu).T * dmu_dtheta.T * w + V\n",
    "        return Q\n",
    "\n",
    "    def Q_grad_a_sym(self):\n",
    "        \"\"\"defines symbolically gradient (wrt action) of the action-value function\"\"\"\n",
    "        \n",
    "        s, theta, v, a, w = self.symbols_list()\n",
    "        a = sp.Matrix([item for item in a])\n",
    "        Q = self.Q_func_sym()\n",
    "        dQ_da = Q.jacobian(a).T\n",
    "        return dQ_da\n",
    "\n",
    "    def Q_grad_w_sym(self):\n",
    "        \"\"\"\n",
    "        defines symbolically gradient (wrt its parameters) of the action-value\n",
    "        function\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        s, theta, v, a, w = self.symbols_list()\n",
    "        w = sp.Matrix([item for item in w])\n",
    "        Q = self.Q_func_sym()\n",
    "        dQ_dw = Q.jacobian(w).T\n",
    "        return dQ_dw\n",
    "\n",
    "    def lambdify_funcs(self):\n",
    "        \"\"\"Lambdifies the functions of interest with numpy as the target library\"\"\"\n",
    "        \n",
    "        s, theta, v, a, w = self.symbols_list()\n",
    "        mu_lambdified = sp.lambdify([s, theta], self.mu_func_sym(), 'numpy')\n",
    "        V_lambdified = sp.lambdify([s, v], self.V_func_sym(), 'numpy')\n",
    "        Q_lambdified = sp.lambdify([s, a, theta, v, w], self.Q_func_sym(),\n",
    "                                   'numpy')\n",
    "        mu_grad_lambdified = sp.lambdify([s, theta], self.mu_grad_sym(),\n",
    "                                         'numpy')\n",
    "        V_grad_lambdified = sp.lambdify([s, v], self.V_grad_sym(), 'numpy')\n",
    "        Q_grad_a_lambdified = sp.lambdify([s, a, theta, v, w],\n",
    "                                          self.Q_grad_a_sym(), 'numpy')\n",
    "        Q_grad_w_lambdified = sp.lambdify([s, a, theta, v, w],\n",
    "                                          self.Q_grad_w_sym(), 'numpy')\n",
    "        return mu_lambdified, V_lambdified, Q_lambdified, mu_grad_lambdified,\\\n",
    "               V_grad_lambdified, Q_grad_a_lambdified, Q_grad_w_lambdified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SmartBuildings",
   "language": "python",
   "name": "smartbuildings"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
