{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     6
    ]
   },
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class Simple1Room:\n",
    "    \"\"\"\n",
    "    A class used to represent a simple 1-zone building\n",
    "    \n",
    "    ...\n",
    "    Attributes\n",
    "    ----------\n",
    "    Td: float\n",
    "        desired temperature\n",
    "    To: float\n",
    "        outdoor temperature (default -10)\n",
    "    Qh: float\n",
    "        the specific heater power (default 1.3*10**4/(2*10**6))\n",
    "    relect: float\n",
    "        reward/penalty for electricity use (default -1*0.12*10/3600)\n",
    "    rcom: float\n",
    "        reward/penalty for temperature comfort violation \n",
    "        (default -1*0.12*10/3600)\n",
    "    rsw: float\n",
    "        reward/penalty for switching heater ON/OFF (default -1.1)\n",
    "    alpha: float\n",
    "        system dynamic coefficient (default -1.3*250/(2*10**6))\n",
    "    dt: int\n",
    "        the simulation time step in seconds (defalt 25)\n",
    "    dt_TransStep: int\n",
    "        MDP transition intervals in minuts for fixed-time transition \n",
    "        mode (default 5)\n",
    "    fixed_TransTime: bool\n",
    "        if set to True, fixed transitions with intervals of dt_TransStep\n",
    "        minutes is used (default False)\n",
    "    time_out: int\n",
    "        is the longest time in hour till which if a transition does not \n",
    "        happen, simulation is terminated (default 5)\n",
    "    \n",
    "    Methods\n",
    "    -------\n",
    "    reset()\n",
    "        resets the system state of (T0, hs0, aT0, zT0)\n",
    "    step(action)\n",
    "        takes input action (temp. threshold) and simulates the system \n",
    "        for one transition step\n",
    "    _RaveCalc(self, theta_mu, theta_sigma, T_lowerbound, T_upperbound, \n",
    "              max_iter, deterministic = True, plots = False,\n",
    "              PerTimeStep = False)\n",
    "        Calculates the average reward for a given input policy\n",
    "    comf_rate(T,Td)\n",
    "        calculates how the deviation from Td is penalized for reward\n",
    "        calculation\n",
    "    get_action(theta_mu, theta_sigma, T_lowerbound, T_upperbound,\n",
    "               deterministic)\n",
    "        caluclates a temperature threshold action for the use in \n",
    "        _RaveCalc()\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 Td,\n",
    "                 To=-10.0,\n",
    "                 relect=-1 * 0.12 * 10 / 3600,\n",
    "                 rcom=-1 * 0.12 * 10 / 3600,\n",
    "                 rsw=-1.1,\n",
    "                 alpha=-1.3 * 250 / (2 * 10**6),\n",
    "                 dt=25,\n",
    "                 dt_TransStep=5,\n",
    "                 fixed_TransTime=False,\n",
    "                 Qh=1.3 * 10**4 / (2 * 10**6),\n",
    "                 time_out=5):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        Td: float\n",
    "            desired temperature\n",
    "        To: float, optional\n",
    "            outdoor temperature (default -10)\n",
    "        Qh: float, optional\n",
    "            the specific heater power (default 1.3*10**4/(2*10**6))\n",
    "        relect: float, optional\n",
    "            reward/penalty for electricity use (default -1*0.12*10/3600)\n",
    "        rcom: float, optional\n",
    "            reward/penalty for temperature comfort violation \n",
    "            (default -1*0.12*10/3600)\n",
    "        rsw: float, optional\n",
    "            reward/penalty for switching heater ON/OFF (default -1.1)\n",
    "        alpha: float, optional\n",
    "            system dynamic coefficient (default -1.3*250/(2*10**6))\n",
    "        dt: int, optional\n",
    "            the simulation time step in seconds (defalt 25)\n",
    "        dt_TransStep: int, optional\n",
    "            MDP transition intervals in minuts for fixed-time transition \n",
    "            mode (default 5)\n",
    "        fixed_TransTime: bool, optional\n",
    "            if set to True, fixed transitions with intervals of dt_TransStep\n",
    "            minutes is used (default False)\n",
    "        time_out: int, optional\n",
    "            is the longest time in hours till which if a transition does not \n",
    "            happen, simulation is terminated (default is 5)\n",
    "        \"\"\"\n",
    "\n",
    "        #         Instantiating the envirobment model with the thermodynamics and\n",
    "        #         rewards parameters:\n",
    "\n",
    "        #         relec:  reward coefficient for heater being on: R'=-relec*s[t]-rcom(T-Td)^2\n",
    "\n",
    "        self.Td = Td\n",
    "        self.To = To\n",
    "        self.relect = relect\n",
    "        self.rcom = rcom\n",
    "        self.rsw = rsw\n",
    "        self.alpha = alpha\n",
    "        self.Qh = Qh\n",
    "        self.dt = dt\n",
    "        self.dt_TransStep = dt_TransStep * 60\n",
    "        self.time_out = time_out * 3600\n",
    "        self.fixed_TransTime = fixed_TransTime\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        resets the system state of (T0, hs0, aT0, zT0)\n",
    "        \n",
    "        resets indoor temp. (T0) to a value equal to Td plus a random\n",
    "        noise uniformly distributed in [-1,1]. \n",
    "        resets the heater status (hs0) randomly to 0 or 1\n",
    "        resets action (aT0), the temp. threshold to Td\n",
    "        resets zT0 (the state which shows if we the temp. has just \n",
    "        reached the threshold if its value is 1) to 1.\n",
    "        \"\"\"\n",
    "\n",
    "        T0 = self.Td + np.random.uniform(low=-1, high=1)\n",
    "        hs0 = np.random.choice([0, 1])\n",
    "        aT0 = self.Td\n",
    "        zT0 = 1\n",
    "        self.state = (T0, hs0, aT0, zT0)\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        takes input action (temp. threshold) and simulates the system \n",
    "        for one transition step\n",
    "        \n",
    "        Note that aT is the action taken at the previous step.\n",
    "        this function returns the new system state, the total reward and\n",
    "        time spent during that step as well as time series of temp,\n",
    "        reward, and time spent in that step. It also returns a variable\n",
    "        named \"done\" which indicates the end of total simulation if its\n",
    "        value is set to True.\n",
    "        \"\"\"\n",
    "\n",
    "        state = self.state\n",
    "        Tth = action\n",
    "\n",
    "        T, hs, aT, zT = state\n",
    "        zT = 0\n",
    "        reward = 0\n",
    "        dt_step = 0\n",
    "        done = False\n",
    "        loop_condition = True\n",
    "        T_StepTimeSeries = []\n",
    "        reward_StepTimeSeries = []\n",
    "        t_StepTimeSeries = []\n",
    "\n",
    "        while loop_condition:\n",
    "\n",
    "            dT = self.alpha * (T - self.To) + self.Qh * hs\n",
    "            dr = self.relect * hs + self.rcom * self.comf_rate(T, self.Td)\n",
    "\n",
    "            T = T + dT * self.dt\n",
    "            reward = reward + dr * self.dt\n",
    "            dt_step = dt_step + self.dt\n",
    "\n",
    "            T_StepTimeSeries.append(T)\n",
    "            reward_StepTimeSeries.append(reward)\n",
    "            t_StepTimeSeries.append(dt_step)\n",
    "\n",
    "            TempThreshNotReached = (hs == 0 and T >= Tth) or (hs == 1\n",
    "                                                              and T <= Tth)\n",
    "            FixedTransTimeNotReached = (dt_step <= self.dt_TransStep)\n",
    "\n",
    "            if self.fixed_TransTime:\n",
    "                loop_condition = FixedTransTimeNotReached\n",
    "            else:\n",
    "                loop_condition = TempThreshNotReached\n",
    "\n",
    "            if dt_step > self.time_out:\n",
    "                done = True\n",
    "                break\n",
    "\n",
    "        if not reward_StepTimeSeries:\n",
    "            print(\"reward_StepTimeSeries is empty!\")\n",
    "            print(\"T:{} \".format(T))\n",
    "            print(\"hs:{} \".format(hs))\n",
    "            print(\"Tth:{} \".format(Tth))\n",
    "\n",
    "        if not TempThreshNotReached:\n",
    "            zT = 1\n",
    "            hs = 1 - hs\n",
    "            reward = reward + self.rsw\n",
    "\n",
    "        aT = Tth\n",
    "        reward_StepTimeSeries[-1] = reward\n",
    "\n",
    "        self.state = (T, hs, aT, zT)\n",
    "        return self.state, reward, T_StepTimeSeries, reward_StepTimeSeries,\\\n",
    "               dt_step, t_StepTimeSeries, done\n",
    "\n",
    "    @staticmethod\n",
    "    def comf_rate(T, Td):\n",
    "        \"\"\"\n",
    "        calculates how the deviation from Td is penalized for reward\n",
    "        calculation\n",
    "        \n",
    "        if |T-Td|<1 no penalty is considered for comfort violation;\n",
    "        otherwise (|T-Td|-1)^2 is used to penalize the comfort violation\n",
    "        \"\"\"\n",
    "\n",
    "        if abs(T - Td) < 1:\n",
    "            r_comf_rate = 0\n",
    "        else:\n",
    "            r_comf_rate = (abs(T - Td) - 1)**2\n",
    "        return r_comf_rate\n",
    "\n",
    "    def _RaveCalc(self,\n",
    "                  theta_mu,\n",
    "                  theta_sigma,\n",
    "                  T_lowerbound,\n",
    "                  T_upperbound,\n",
    "                  deterministic,\n",
    "                  max_iter=1000,\n",
    "                  max_hour=5*24.0,\n",
    "                  plots=False,\n",
    "                  PerTimeStep=True):\n",
    "        \"\"\"\n",
    "        Calculates the average reward for a given input policy in the\n",
    "        environment\n",
    "        \n",
    "        \n",
    "        parameters\n",
    "        ----------\n",
    "        \n",
    "        theta_mu: numpy array \n",
    "            [theta_ON, theta_OFF] parameters for the mean value of \n",
    "            temp. thresholds for ON/OFF switching. If deterministic=True\n",
    "            the mean temp. thresholds calculated using these parameters\n",
    "            are used as the actual deterministic threshold actions.\n",
    "            Otherwise if deterministic=False, the said calculated \n",
    "            thresholds are used as the mean of the Gaussian distribution\n",
    "            for calculating temp. threshold actions.\n",
    "        theta_sigma: numpy float\n",
    "            if deterministic=True this is ignored, otherwise it is used\n",
    "            as the standard deviation of the Gaussian distribution\n",
    "            for calculating temp. threshold actions.\n",
    "        T_lowerbound: float\n",
    "            minimum temperature allowed when sampling the Gaussian dist.\n",
    "            of the switching-ON temperature threshold when \n",
    "            deterministic=False\n",
    "        T_upperbound: float\n",
    "            maximum temperature allowed when sampling the Gaussian dist.\n",
    "            of the switching-OFF temperature threshold when \n",
    "            deterministic=False\n",
    "        max_iter: int\n",
    "            maximum number of transition steps for simulation\n",
    "        max_hour: float\n",
    "            maximum number of hours for simulation\n",
    "        deterministic: bool\n",
    "            if True, it calculates the average reward for a \n",
    "            deterministic policy in which temperature thresholds are \n",
    "            defined by the parameters theta_mu. if False it calculates\n",
    "            the average reward for a stochastic policy in which \n",
    "            temperature thresholds are defined by the parameters \n",
    "            theta_mu and theta_sigma\n",
    "        plots: bool, optional\n",
    "            if set to True it plots a bunch of related plots (default\n",
    "            False)\n",
    "        PerTimeStep: bool\n",
    "            if set to True, average reward is calculated as divding the\n",
    "            total reward by the total time; otherwise it is calculated\n",
    "            as divding the total reward by the total number of \n",
    "            (transition) steps \n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        S = self.reset()\n",
    "\n",
    "        G_stepseries, T_stepseries, t_stepseries, hs_stepseries, G_timeseries,\\\n",
    "        T_timeseries, t_timeseries = [],[],[],[],[],[],[]\n",
    "        \n",
    "        G_stepseries.append(0)\n",
    "        T_stepseries.append(S[0])\n",
    "        t_stepseries.append(0)\n",
    "        hs_stepseries.append(S[1])\n",
    "        G_timeseries.append(0)\n",
    "        T_timeseries.append(S[0])\n",
    "        t_timeseries.append(0)\n",
    "\n",
    "        ind = 0\n",
    "        for i in range(max_iter):\n",
    "\n",
    "            A = self.get_action(theta_mu, theta_sigma, T_lowerbound,\n",
    "                                T_upperbound, deterministic)\n",
    "\n",
    "            if i == 0:\n",
    "                # making sure the system does not go unstable in the very \n",
    "                # beginning\n",
    "                if S[1] == 1 and S[0] > A:\n",
    "                    A = S[0] + 0.5\n",
    "                if S[1] == 0 and S[0] < A:\n",
    "                    A = S[0] - 0.5\n",
    "\n",
    "            Sp, R, T_StepTimeSeries, reward_StepTimeSeries, dt_step,\\\n",
    "            t_StepTimeSeries, done = self.step(A)\n",
    "            S = Sp\n",
    "\n",
    "            ind = ind + 1\n",
    "            T_stepseries.append(S[0])\n",
    "            hs_stepseries.append(S[1])\n",
    "            G_stepseries.append(G_stepseries[-1] + R)\n",
    "            t_stepseries.append(t_stepseries[-1] + dt_step)\n",
    "\n",
    "            T_timeseries.extend(T_StepTimeSeries)\n",
    "            G_timeseries.extend(\n",
    "                map(lambda x: x + G_stepseries[-2], reward_StepTimeSeries))\n",
    "            t_timeseries.extend(\n",
    "                map(lambda x: x + t_stepseries[-2], t_StepTimeSeries))\n",
    "            \n",
    "            if (t_timeseries[-1]/3600)>max_hour:\n",
    "                break\n",
    "\n",
    "        t_stepseries[:] = [x / 3600 for x in t_stepseries]\n",
    "        t_timeseries[:] = [x / 3600 for x in t_timeseries]\n",
    "\n",
    "        if PerTimeStep:\n",
    "            Rave = G_stepseries[-1] / t_stepseries[-1]  # Rave (per hour)\n",
    "        else:\n",
    "            Rave = G_stepseries[-1] / ind  # Rave (per step)\n",
    "\n",
    "        if plots:\n",
    "            fig, ax1 = plt.subplots(figsize=(8, 6))\n",
    "            color = 'tab:blue'\n",
    "            ax1.set_xlabel('time (hour)')\n",
    "            ax1.set_ylabel('total reward')\n",
    "            ax1.plot(t_stepseries, G_stepseries, color=color)\n",
    "            ax1.tick_params(axis='y')\n",
    "            fig.tight_layout()\n",
    "\n",
    "            fig, ax1 = plt.subplots(figsize=(8, 6))\n",
    "            color = 'tab:blue'\n",
    "            ax1.set_xlabel('time (hour)')\n",
    "            ax1.set_ylabel('temperature')\n",
    "            ax1.plot(t_timeseries, T_timeseries, color=color)\n",
    "            ax1.tick_params(axis='y')\n",
    "            fig.tight_layout()\n",
    "\n",
    "            if PerTimeStep:\n",
    "                print (\"Running the trained policy:\\n Total Reward = {}\\n total time = {} hours \\n R_average (per hour) = {}\"\\\n",
    "                       .format(G_stepseries[-1], t_stepseries[-1], Rave))\n",
    "            else:\n",
    "                print(\"Running the trained policy:\\n Total Reward = {}\\n # of steps = {}\\n R_average = {}\".\n",
    "                      format(G_stepseries[-1], ind, Rave))\n",
    "\n",
    "        return Rave\n",
    "\n",
    "    def get_action(self, theta_mu, theta_sigma, T_lowerbound, T_upperbound,\n",
    "                   deterministic):\n",
    "        \"\"\"\n",
    "        caluclates a temperature threshold action for the use in \n",
    "        _RaveCalc() based on the input parameters\n",
    "        \"\"\"\n",
    "        \n",
    "        # choosing action (threshold temperatures)\n",
    "        T, hs, aT, zT = self.state\n",
    "        feature_vec = np.array([1 - hs, hs])\n",
    "        #         feature_vec = PolicyGradient_AC.feature_vec(self.state)\n",
    "        mu = np.dot(theta_mu, feature_vec)\n",
    "        sigma = np.exp(theta_sigma)\n",
    "\n",
    "        if deterministic:\n",
    "            action = mu\n",
    "        else:\n",
    "\n",
    "            while True:\n",
    "                action = np.random.normal(mu, sigma)\n",
    "                if (hs == 1 and action > T and action < T_upperbound) or (\n",
    "                        hs == 0 and action < T and action > T_lowerbound):\n",
    "                    break\n",
    "\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class offline_sim:\n",
    "    \n",
    "    def __init__(self,\n",
    "                 model,\n",
    "                 scaler,\n",
    "                 policy):\n",
    "        \n",
    "        self.model=model\n",
    "        self.scaler=scaler\n",
    "        self.policy=policy\n",
    "    \n",
    "    def reset(self):\n",
    "        T0 = self.policy[0]\n",
    "        hs0 = 1.0\n",
    "        self.state = (T0, hs0)\n",
    "        return self.state\n",
    "    \n",
    "    def choose_action(self, state):\n",
    "        T, hs = state\n",
    "        if hs==0.0:\n",
    "            action=self.policy[0]\n",
    "        elif hs==1.0:\n",
    "            action=self.policy[1]\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def step(self, action):\n",
    "\n",
    "        state = self.state\n",
    "        T, hs = state\n",
    "        Tth = action\n",
    "        \n",
    "        if hs==0:\n",
    "            model=self.model[0]\n",
    "            scaler=self.scaler[0]\n",
    "        elif hs==1:\n",
    "            model=self.model[1]\n",
    "            scaler=self.scaler[1]\n",
    "        \n",
    "        scaler_X=scaler[0]\n",
    "        scaler_y=scaler[1]\n",
    "        \n",
    "        X=np.array([[T, Tth]])\n",
    "        X_sc = scaler_X.transform(X)\n",
    "        predict = model.predict(X_sc)\n",
    "        predict=scaler_y.inverse_transform(predict)\n",
    "        \n",
    "        r=predict[0,0]\n",
    "        tau=predict[0,1]\n",
    "        T=Tth\n",
    "        hs=1-hs\n",
    "        \n",
    "        self.state = (T, hs)\n",
    "        return self.state, r, tau\n",
    "    \n",
    "    def _calculate_rave(self, max_iter=10):\n",
    "        S = self.reset()\n",
    "        trans_tuple=[]\n",
    "        \n",
    "        for step_iter in range(max_iter):\n",
    "            \n",
    "            A = self.choose_action(S)\n",
    "            Sp, r, tau = self.step(A)\n",
    "            trans_tuple.append([S,A,Sp,r,tau])\n",
    "            S = Sp\n",
    "        \n",
    "        trans_tuple = np.asarray(trans_tuple)\n",
    "        rave = trans_tuple[:,3].sum()/trans_tuple[:,4].sum()\n",
    "        return rave*3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import import_ipynb\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Simple1Room_gym(gym.Env):\n",
    "    \"\"\"\n",
    "    A gym-compatible class used to represent a simple 1-zone building\n",
    "    \n",
    "    ...\n",
    "    Attributes\n",
    "    ----------\n",
    "    Td: float\n",
    "        desired temperature\n",
    "    To: float\n",
    "        outdoor temperature (default -10)\n",
    "    Qh: float\n",
    "        the specific heater power (default 1.3*10**4/(2*10**6))\n",
    "    relect: float\n",
    "        reward/penalty for electricity use (default -1*0.12*10/3600)\n",
    "    rcom: float\n",
    "        reward/penalty for temperature comfort violation \n",
    "        (default -1*0.12*10/3600)\n",
    "    rsw: float\n",
    "        reward/penalty for switching heater ON/OFF (default -1.1)\n",
    "    alpha: float\n",
    "        system dynamic coefficient (default -1.3*250/(2*10**6))\n",
    "    dt: int\n",
    "        the simulation time step in seconds (defalt 25)\n",
    "    dt_TransStep: int\n",
    "        MDP transition intervals in minuts for fixed-time transition \n",
    "        mode (default 5)\n",
    "    fixed_TransTime: bool\n",
    "        if set to True, fixed transitions with intervals of dt_TransStep\n",
    "        minutes is used (default False)\n",
    "    time_out: int\n",
    "        is the longest time in hour till which if a transition does not \n",
    "        happen, simulation is terminated (default 5)\n",
    "    \n",
    "    Methods\n",
    "    -------\n",
    "    reset()\n",
    "        resets the system state of (T0, hs0, aT0, zT0)\n",
    "    step(action)\n",
    "        takes input action (temp. threshold) and simulates the system \n",
    "        for one transition step\n",
    "    _RaveCalc(self, theta_mu, theta_sigma, T_lowerbound, T_upperbound, \n",
    "              max_iter, deterministic = True, plots = False,\n",
    "              PerTimeStep = False)\n",
    "        Calculates the average reward for a given input policy\n",
    "    comf_rate(T,Td)\n",
    "        calculates how the deviation from Td is penalized for reward\n",
    "        calculation\n",
    "    get_action(theta_mu, theta_sigma, T_lowerbound, T_upperbound,\n",
    "               deterministic)\n",
    "        caluclates a temperature threshold action for the use in \n",
    "        _RaveCalc()\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 Td,\n",
    "                 To=-10.0,\n",
    "                 relect=-1 * 0.12 * 10 / 3600,\n",
    "                 rcom=-1 * 0.12 * 10 / 3600,\n",
    "                 rsw=-1.1,\n",
    "                 alpha=-1.3 * 250 / (2 * 10**6),\n",
    "                 dt=20,\n",
    "                 dt_TransStep=5,\n",
    "                 fixed_TransTime=True,\n",
    "                 Qh=1.3 * 10**4 / (2 * 10**6),\n",
    "                 time_max = 24,\n",
    "                 time_out=5,\n",
    "                 num_envs = 1):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        Td: float\n",
    "            desired temperature\n",
    "        To: float, optional\n",
    "            outdoor temperature (default -10)\n",
    "        Qh: float, optional\n",
    "            the specific heater power (default 1.3*10**4/(2*10**6))\n",
    "        relect: float, optional\n",
    "            reward/penalty for electricity use (default -1*0.12*10/3600)\n",
    "        rcom: float, optional\n",
    "            reward/penalty for temperature comfort violation \n",
    "            (default -1*0.12*10/3600)\n",
    "        rsw: float, optional\n",
    "            reward/penalty for switching heater ON/OFF (default -1.1)\n",
    "        alpha: float, optional\n",
    "            system dynamic coefficient (default -1.3*250/(2*10**6))\n",
    "        dt: int, optional\n",
    "            the simulation time step in seconds (defalt 25)\n",
    "        dt_TransStep: int, optional\n",
    "            MDP transition intervals in minuts for fixed-time transition \n",
    "            mode (default 5)\n",
    "        fixed_TransTime: bool, optional\n",
    "            if set to True, fixed transitions with intervals of dt_TransStep\n",
    "            minutes is used (default False)\n",
    "        time_out: int, optional\n",
    "            is the longest time in hours till which if a transition does not \n",
    "            happen, simulation is terminated (default is 5)\n",
    "        \"\"\"\n",
    "\n",
    "        #         Instantiating the envirobment model with the thermodynamics and\n",
    "        #         rewards parameters:\n",
    "\n",
    "        #         relec:  reward coefficient for heater being on: R'=-relec*s[t]-rcom(T-Td)^2\n",
    "\n",
    "\n",
    "        self.action_space = gym.spaces.Box(low = -1,\n",
    "                                           high = 1, shape = (1,), dtype=np.float32)\n",
    "        self.observation_space = gym.spaces.Box(low = np.array([Td-10.0, 0]),\n",
    "                                                high = np.array([Td+10.0, 1]), dtype=np.float32)\n",
    "        self.Td = Td\n",
    "        self.To = To\n",
    "        self.relect = relect\n",
    "        self.rcom = rcom\n",
    "        self.rsw = rsw\n",
    "        self.alpha = alpha\n",
    "        self.Qh = Qh\n",
    "        self.dt = dt\n",
    "        self.dt_TransStep = dt_TransStep * 60\n",
    "        self.time_out = time_out * 3600\n",
    "        self.fixed_TransTime = fixed_TransTime\n",
    "        self.time = 0.0\n",
    "        self.time_max = time_max * 3600\n",
    "        self.Toffup = Td+5\n",
    "        self.Tofflow = Td+1\n",
    "        self.Tonup = Td-2\n",
    "        self.Tonlow = Td-7\n",
    "        self.num_envs = num_envs\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        resets the system state of (T0, hs0)\n",
    "        \n",
    "        resets indoor temp. (T0) to a value equal to Td plus a random\n",
    "        noise uniformly distributed in [-1,1]. \n",
    "        resets the heater status (hs0) randomly to 0 or 1\n",
    "        resets action (aT0), the temp. threshold to Td\n",
    "        resets zT0 (the state which shows if we the temp. has just \n",
    "        reached the threshold if its value is 1) to 1.\n",
    "        \"\"\"\n",
    "\n",
    "        T0 = self.Td + np.random.uniform(low=-1, high=1)\n",
    "        hs0 = np.random.choice([0, 1])\n",
    "        self.state = np.array([T0, hs0])\n",
    "        self.time = 0\n",
    "        self.cumrew = 0\n",
    "        obs = self.state\n",
    "        return obs\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        takes input action (temp. threshold) and simulates the system \n",
    "        for one transition step\n",
    "        \n",
    "        Note that aT is the action taken at the previous step.\n",
    "        this function returns the new system state, the total reward and\n",
    "        time spent during that step as well as time series of temp,\n",
    "        reward, and time spent in that step. It also returns a variable\n",
    "        named \"done\" which indicates the end of total simulation if its\n",
    "        value is set to True.\n",
    "        \"\"\"\n",
    "\n",
    "        state = self.state\n",
    "        Ton_scaled = action\n",
    "        Toff = 16.0\n",
    "        Ton = self.denormalize_action_func(self.Tonup,self.Tonlow,Ton_scaled)\n",
    "\n",
    "        T, hs = state\n",
    "        reward = 0\n",
    "        dt_step = 0\n",
    "        done = False\n",
    "        loop_condition = True\n",
    "\n",
    "        while loop_condition:\n",
    "\n",
    "            dT = self.alpha * (T - self.To) + self.Qh * hs\n",
    "            dr = self.relect * hs + self.rcom * self.comf_rate(T, self.Td)\n",
    "\n",
    "            T = T + dT * self.dt\n",
    "            reward = reward + dr * self.dt\n",
    "            dt_step = dt_step + self.dt\n",
    "            self.time = self.time + self.dt\n",
    "\n",
    "            if dt_step>=self.dt_TransStep:\n",
    "                loop_condition = False\n",
    "                TempThreshReached = (hs == 0 and T <= Ton) or (hs == 1\n",
    "                                                              and T >= Toff)\n",
    "                if TempThreshReached:\n",
    "                    hs = 1 - hs\n",
    "                    reward = reward + self.rsw\n",
    "                \n",
    "            if self.time >= self.time_max:\n",
    "                done = True\n",
    "\n",
    "        self.cumrew = self.cumrew + reward \n",
    "        self.state = np.array([T, hs])\n",
    "        obs = self.state\n",
    "        info = {}\n",
    "        return obs, reward, done, info\n",
    "\n",
    "    @staticmethod\n",
    "    def comf_rate(T, Td):\n",
    "        \"\"\"\n",
    "        calculates how the deviation from Td is penalized for reward\n",
    "        calculation\n",
    "        \n",
    "        if |T-Td|<1 no penalty is considered for comfort violation;\n",
    "        otherwise (|T-Td|-1)^2 is used to penalize the comfort violation\n",
    "        \"\"\"\n",
    "\n",
    "        if abs(T - Td) < 1:\n",
    "            r_comf_rate = 0\n",
    "        else:\n",
    "            r_comf_rate = (abs(T - Td) - 1)**2\n",
    "        return r_comf_rate\n",
    "    \n",
    "    def denormalize_action_func(self,upbound, lowbound,a):\n",
    "        denormalized_a = (upbound-lowbound)*(a+1)/2+lowbound\n",
    "        return denormalized_a\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SmartBuildings",
   "language": "python",
   "name": "smartbuildings"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
